# üéß Rule Based Voice vs Noise Detection

This repository contains a lightweight Python script that segments a one minute audio file into one second intervals and classifies each as either human voice or non speech noise using a fully rule based system with no machine learning required.

This project simulates a lightweight audio pre processor suitable for filtering scientific voice data or embedded voice logs.

---

## üìå Overview

**Goal:**  
Classify one second audio segments as voice or noise using frequency domain analysis and interpretable rules.

**Key Features**  
‚Ä¢ Log normalized voice band energy ratio  
‚Ä¢ Spectral energy  
‚Ä¢ Spectral flatness  
‚Ä¢ Fundamental pitch estimation using Parselmouth  
‚Ä¢ Voicing probability

---

## üìÇ Input

The script takes a mono WAV audio file approximately one minute long that contains a mix of:

‚Ä¢ Clear human speech  
‚Ä¢ Background noise including crowds, keyboard typing, fans, or ambient hum

You can use a preexisting audio file or create your own using the provided `record.py` script:

```bash
python record.py
```

---

## üéõÔ∏è Feature Extraction

Each one second segment is analyzed using the following features to identify the presence of voice:

**Total Spectral Energy**  
Captures the overall power of the signal. Human speech generally carries more energy than ambient noise.

**Spectral Flatness**  
Indicates how noise like the signal is. High flatness suggests white noise or static, while low flatness indicates more tonal structure as seen in speech.

**Fundamental Pitch (F‚ÇÄ)**  
Reflects the base frequency of vocal fold vibration, a strong marker of human speech.

**Voicing Probability**  
Represents the percentage of time the signal is voiced. High voicing means more likelihood of speech being present.

**Voice Band Energy Ratio (Log Normalized)**  
Measures how much energy is present in the typical human speech range between 300 and 3400 Hz, relative to total energy. The result is log scaled to reduce sensitivity to outliers and better capture proportional differences.

---

## üß† Classification Logic

Each segment receives a score based on the extracted features. A segment is labeled as voice only if the combined score passes a configurable threshold.

Scoring rules are based on:  
‚Ä¢ Minimum energy  
‚Ä¢ Flatness below a threshold  
‚Ä¢ Valid pitch range  
‚Ä¢ Sufficient voicing probability  
‚Ä¢ Voice band energy ratio above threshold

These rules are tuned for real world recordings.

---

## üß™ Output

The script outputs a structured list of labeled segments in JSON format:

```json
[
  {"start_time": 0, "end_time": 1, "label": "voice"},
  {"start_time": 1, "end_time": 2, "label": "noise"}
]
```

The terminal also prints a detailed feature summary for each segment with colored indicators showing which thresholds were met and you can visualize feature trends using the time series plots for deeper analysis.

## üöÄ How to Run

**Step 1: Clone the Repository**
```bash
git clone https://github.com/your-username/vad-rule-based.git
cd vad-rule-based
```
**Step 2: Install Python Dependencies**

```bash
pip install -r requirements.txt
```

**Step 3: Add Your Audio File**
Place your audio file inside the recordings folder. Example:

```bash
recordings/recording.wav
```
**Step 4: Run the Script**

```bash
python audioSegmentation.py
```
## üìã Assumptions and Limitations

‚Ä¢ Input must be mono audio sampled at 16000 Hz  
‚Ä¢ The method is not designed for overlapping speakers or music with vocals  
‚Ä¢ Thresholds are empirically tuned and may require adjustment for different environments  
‚Ä¢ Built for interpretability and lightweight performance without any machine learning  

## üì¨ Contact
Created by Mekhael Thaha
